{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from typing import Dict, Any\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"allenai/qasper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw bar plot of the number of questions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_questions = [len(dataset['train'][i]['qas']['question']) for i in range(len(dataset['train']))]\n",
    "plt.hist(num_questions, bins=12, range=(0.5, 12.5))\n",
    "plt.title('Number of questions per article')\n",
    "plt.xlabel('Number of questions')\n",
    "plt.ylabel('Number of article')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw histogram of average number of answers per question\n",
    "num_avgs = []\n",
    "all_answer_counts = []\n",
    "num_free_form_answers = 0\n",
    "num_extractive_answers = 0\n",
    "num_unanswerable = 0\n",
    "abstract_lengths = []\n",
    "for article in dataset['train']:\n",
    "    abstract_lengths.append(len(article['abstract'].split(' ')))\n",
    "    for answers in article['qas']['answers']:\n",
    "        for answer_dict in answers['answer']:\n",
    "            if answer_dict['free_form_answer'] != '':\n",
    "                num_free_form_answers += 1\n",
    "            if len(answer_dict['extractive_spans']) != 0:\n",
    "                num_extractive_answers += 1\n",
    "            if answer_dict['unanswerable'] == True:\n",
    "                num_unanswerable += 1\n",
    "    answer_counts = [len(answers['answer']) for answers in article['qas']['answers']]\n",
    "    num_avgs.append(sum(answer_counts) / len(answer_counts))\n",
    "    all_answer_counts.extend(answer_counts)\n",
    "\n",
    "print('Number of answers: ', sum(all_answer_counts))\n",
    "print('Number of free form answers: ', num_free_form_answers)\n",
    "print('Number of extractive answers: ', num_extractive_answers)\n",
    "print('Number of unanswerable questions: ', num_unanswerable)\n",
    "print('Average number of answers per question: ', sum(all_answer_counts) / len(all_answer_counts))\n",
    "plt.hist(num_avgs)\n",
    "plt.title('Average number of answers per question')\n",
    "plt.xlabel('Average number of answers')\n",
    "plt.ylabel('Number of articles')\n",
    "plt.show()\n",
    "\n",
    "# draw histogram of abstract lengths\n",
    "plt.hist(abstract_lengths, bins=50, range=(0, 300))\n",
    "plt.title('Abstract lengths in words')\n",
    "plt.xlabel('Abstract length (in words)')\n",
    "plt.ylabel('Number of articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_questions = [len(dataset['test'][i]['qas']['question']) for i in range(len(dataset['test']))]\n",
    "print('Number of questions in test set: ', sum(num_questions)) \n",
    "plt.hist(num_questions, bins=12, range=(0.5, 12.5))\n",
    "plt.title('Number of questions per article')\n",
    "plt.xlabel('Number of questions')\n",
    "plt.ylabel('Number of article')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw histogram of average number of answers per question\n",
    "num_avgs = []\n",
    "all_answer_counts = []\n",
    "num_free_form_answers = 0\n",
    "num_extractive_answers = 0\n",
    "num_unanswerable = 0\n",
    "abstract_lengths = []\n",
    "for article in dataset['test']:\n",
    "    abstract_lengths.append(len(article['abstract'].split(' ')))\n",
    "    for answers in article['qas']['answers']:\n",
    "        for answer_dict in answers['answer']:\n",
    "            if answer_dict['free_form_answer'] != '':\n",
    "                num_free_form_answers += 1\n",
    "            if len(answer_dict['extractive_spans']) != 0:\n",
    "                num_extractive_answers += 1\n",
    "            if answer_dict['unanswerable'] == True:\n",
    "                num_unanswerable += 1\n",
    "    answer_counts = [len(answers['answer']) for answers in article['qas']['answers']]\n",
    "    num_avgs.append(sum(answer_counts) / len(answer_counts))\n",
    "    all_answer_counts.extend(answer_counts)\n",
    "\n",
    "print('Number of answers: ', sum(all_answer_counts))\n",
    "print('Number of free form answers: ', num_free_form_answers)\n",
    "print('Number of extractive answers: ', num_extractive_answers)\n",
    "print('Number of unanswerable questions: ', num_unanswerable)\n",
    "print('Average number of answers per question: ', sum(all_answer_counts) / len(all_answer_counts))\n",
    "plt.hist(num_avgs)\n",
    "plt.title('Average number of answers per question')\n",
    "plt.xlabel('Average number of answers')\n",
    "plt.ylabel('Number of articles')\n",
    "plt.show()\n",
    "\n",
    "# draw histogram of abstract lengths\n",
    "plt.hist(abstract_lengths, bins=50, range=(0, 300))\n",
    "plt.title('Abstract lengths in words')\n",
    "plt.xlabel('Abstract length (in words)')\n",
    "plt.ylabel('Number of articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLATTENING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'a', 'f', 'b', 'j', 'd', 'e', 'g', 'h', 'c']\n"
     ]
    }
   ],
   "source": [
    "a = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"]\n",
    "random.shuffle(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the seed lexicon?',\n",
       " 'context': 'Recognizing affective events that trigger positive or negative sentiment has a wide range of natural language processing applications but remains a challenging problem mainly because the polarity of an event is not necessarily predictable from its constituent words. In this paper, we propose to propagate affective polarity using discourse relations. Our method is simple and only requires a very small seed lexicon and a large raw corpus. Our experiments using Japanese data show that our method learns affective events effectively without manually labeled data. It also improves supervised learning results when labeled data are small.',\n",
       " 'answer': 'a vocabulary of positive and negative predicates that helps determine the polarity score of an event'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'How big is the ANTISCAM dataset? ',\n",
       " 'context': 'End-to-end task-oriented dialog models have achieved promising performance on collaborative tasks where users willingly coordinate with the system to complete a given task. While in non-collaborative settings, for example, negotiation and persuasion, users and systems do not share a common goal. As a result, compared to collaborate tasks, people use social content to build rapport and trust in these non-collaborative settings in order to advance their goals. To handle social content, we introduce a hierarchical intent annotation scheme, which can be generalized to different non-collaborative dialog tasks. Building upon TransferTransfo (Wolf et al. 2019), we propose an end-to-end neural network model to generate diverse coherent responses. Our model utilizes intent and semantic slots as the intermediate sentence representation to guide the generation process. In addition, we design a filter to select appropriate responses based on whether these intermediate representations fit the designed task and conversation constraints. Our non-collaborative dialog model guides users to complete the task while simultaneously keeps them engaged. We test our approach on our newly proposed ANTISCAM dataset and an existing PERSUASIONFORGOOD dataset. Both automatic and human evaluations suggest that our model outperforms multiple baselines in these two non-collaborative tasks.',\n",
       " 'answer': '220 human-human dialogs. '}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_sample = {\n",
    "#     'question': xxx,\n",
    "#     'context': xxx,\n",
    "#     'answer': xxx (randomly selected of the possible answers)\n",
    "# }\n",
    "\n",
    "train_samples = []\n",
    "for article in dataset['train']:\n",
    "    for i, question in enumerate(article['qas']['question']):\n",
    "        answers = article['qas']['answers'][i]['answer']\n",
    "        # shuffle answers\n",
    "        random.shuffle(answers)\n",
    "        for answer_dict in answers:\n",
    "            if answer_dict['free_form_answer'] != '':\n",
    "                train_samples.append({\n",
    "                    'question': question,\n",
    "                    'context': article['abstract'],\n",
    "                    'answer': answer_dict['free_form_answer']\n",
    "                })\n",
    "                break\n",
    "            elif len(answer_dict['extractive_spans']) != 0:\n",
    "                train_samples.append({\n",
    "                    'question': question,\n",
    "                    'context': article['abstract'],\n",
    "                    'answer': answer_dict['extractive_spans'][0]\n",
    "                })\n",
    "                break\n",
    "\n",
    "display(train_samples[0])\n",
    "\n",
    "test_samples = []\n",
    "for article in dataset['test']:\n",
    "    for i, question in enumerate(article['qas']['question']):\n",
    "        answers = article['qas']['answers'][i]['answer']\n",
    "        # shuffle answers\n",
    "        random.shuffle(answers)\n",
    "        for answer_dict in answers:\n",
    "            if answer_dict['free_form_answer'] != '':\n",
    "                test_samples.append({\n",
    "                    'question': question,\n",
    "                    'context': article['abstract'],\n",
    "                    'answer': answer_dict['free_form_answer']\n",
    "                })\n",
    "                break\n",
    "            elif len(answer_dict['extractive_spans']) != 0:\n",
    "                test_samples.append({\n",
    "                    'question': question,\n",
    "                    'context': article['abstract'],\n",
    "                    'answer': answer_dict['extractive_spans'][0]\n",
    "                })\n",
    "                break\n",
    "\n",
    "display(test_samples[0])\n",
    "\n",
    "train_abstracts = [sample['context'] for sample in train_samples]\n",
    "train_questions = [sample['question'] for sample in train_samples]\n",
    "train_answers = [sample['answer'] for sample in train_samples]\n",
    "\n",
    "test_abstracts = [sample['context'] for sample in test_samples]\n",
    "test_questions = [sample['question'] for sample in test_samples]\n",
    "test_answers = [sample['answer'] for sample in test_samples]\n",
    "\n",
    "train = {'abstract': train_abstracts, 'question': train_questions, 'answer': train_answers}\n",
    "test = {'abstract': test_abstracts, 'question': test_questions, 'answer': test_answers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': 'Recognizing affective events that trigger positive or negative sentiment has a wide range of natural language processing applications but remains a challenging problem mainly because the polarity of an event is not necessarily predictable from its constituent words. In this paper, we propose to propagate affective polarity using discourse relations. Our method is simple and only requires a very small seed lexicon and a large raw corpus. Our experiments using Japanese data show that our method learns affective events effectively without manually labeled data. It also improves supervised learning results when labeled data are small.',\n",
       " 'question': 'What is the seed lexicon?',\n",
       " 'answer': 'a vocabulary of positive and negative predicates that helps determine the polarity score of an event'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = datasets.Dataset.from_dict(train)\n",
    "test_dataset = datasets.Dataset.from_dict(test)\n",
    "\n",
    "flattened_dataset = datasets.DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "\n",
    "assert len(train_abstracts) == len(train_questions) == len(train_answers)\n",
    "assert len(test_abstracts) == len(test_questions) == len(test_answers)\n",
    "\n",
    "display(flattened_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['abstract', 'question', 'answer'],\n",
       "        num_rows: 1559\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['abstract', 'question', 'answer'],\n",
       "        num_rows: 1167\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['abstract', 'question', 'answer'],\n",
       "        num_rows: 174\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# randomly choose 10% of the training data as validation data from flattened dataset\n",
    "temp = flattened_dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True, seed=SEED)\n",
    "flattened_dataset[\"train\"] = temp[\"train\"]\n",
    "flattened_dataset[\"val\"] = temp[\"test\"]\n",
    "\n",
    "display(flattened_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerome/anaconda3/envs/umlia23/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Notice that since we are working with an encoder-decoder model here, you need to tokenize, truncate and pad the input sequence, i.e., question: {question} context: {abstract}, as well as the output sequence, i.e., answer.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "def preprocess_function(sample: Dict, max_qc_len: int, max_ans_len: int):\n",
    "    \"\"\"\n",
    "    Preprocesses a single sample dictionary, each containing:\n",
    "        'question': question,\n",
    "        'abstract': article['abstract'],\n",
    "        'answer': answer\n",
    "\n",
    "    Tokenizes the question and context, and truncates the concatenation to max_qc_len.\n",
    "    Tokenizes the answer and truncates to max_ans_len.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the question and context pair\n",
    "    qc_pair = tokenizer(\n",
    "        sample[\"question\"],\n",
    "        sample[\"abstract\"],\n",
    "        max_length=max_qc_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Tokenize the answer\n",
    "    ans = tokenizer(\n",
    "        sample[\"answer\"],\n",
    "        max_length=max_ans_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": qc_pair.input_ids,\n",
    "        \"attention_mask\": qc_pair.attention_mask,\n",
    "        \"labels\": ans.input_ids,\n",
    "        \"decoder_attention_mask\": ans.attention_mask,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4574e5a4c5b4c968c8929e8502137d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5db158664f34d9881e456fb0310fa92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308de4ee98324f1ab11d7e64475bd94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/174 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_ds = flattened_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"max_qc_len\": 128, \"max_ans_len\": 32},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load google/t5-efficient-mini\n",
    "from transformers import T5ForConditionalGeneration\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/t5-efficient-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"my-awesome-project\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.001,\n",
    "    # prediction_loss_only=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    output_dir=\"./results_scratch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir='./logs',\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_ds['train'],\n",
    "    eval_dataset=encoded_ds['val'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4xe1fpd7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de963ec015d408e88635b16bff48a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='119.135 MB of 119.135 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▃▃▃▃▄▁▄▁</td></tr><tr><td>eval/runtime</td><td>▁▃▃▆▂▄▄▂█▅</td></tr><tr><td>eval/samples_per_second</td><td>█▆▆▃▇▅▅▇▁▄</td></tr><tr><td>eval/steps_per_second</td><td>█▆▆▃▇▅▅▇▁▄</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▂▂▂▃▃▃▄▁▁▂▂▂▃▃▃▄▄▄</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▂▂▂▃▃▃▄▁▁▂▂▂▃▃▃▄▄▄</td></tr><tr><td>train/learning_rate</td><td>▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁████▇▇▇▆▆████▇▇▆▆▅▅▄</td></tr><tr><td>train/loss</td><td>█▃▄▄▂▃▂▂▃▂▃▃▄▄▃▂▃▃▃▂▃▃▄▂▂▂▂▁▂▃▂▄▃▁▂▂▁▁▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.37552</td></tr><tr><td>eval/runtime</td><td>0.9065</td></tr><tr><td>eval/samples_per_second</td><td>191.953</td></tr><tr><td>eval/steps_per_second</td><td>191.953</td></tr><tr><td>train/epoch</td><td>2.64</td></tr><tr><td>train/global_step</td><td>4120</td></tr><tr><td>train/learning_rate</td><td>0.00023</td></tr><tr><td>train/loss</td><td>2.6091</td></tr><tr><td>train/total_flos</td><td>113043110952960.0</td></tr><tr><td>train/train_loss</td><td>3.052</td></tr><tr><td>train/train_runtime</td><td>174.008</td></tr><tr><td>train/train_samples_per_second</td><td>44.797</td></tr><tr><td>train/train_steps_per_second</td><td>44.797</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-planet-1</strong> at: <a href='https://wandb.ai/nlp_2023_romibomi_und_paulus_maximus/my-awesome-project/runs/4xe1fpd7' target=\"_blank\">https://wandb.ai/nlp_2023_romibomi_und_paulus_maximus/my-awesome-project/runs/4xe1fpd7</a><br/>Synced 6 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230712_153044-4xe1fpd7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4xe1fpd7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfe6ff0a8494e38a15b23e56f8aba4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666878823331596, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Uni_Master/NLP/exec_05/wandb/run-20230712_154033-gd2gmm3z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_2023_romibomi_und_paulus_maximus/my-awesome-project/runs/gd2gmm3z' target=\"_blank\">ethereal-sponge-2</a></strong> to <a href='https://wandb.ai/nlp_2023_romibomi_und_paulus_maximus/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_2023_romibomi_und_paulus_maximus/my-awesome-project' target=\"_blank\">https://wandb.ai/nlp_2023_romibomi_und_paulus_maximus/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_2023_romibomi_und_paulus_maximus/my-awesome-project/runs/gd2gmm3z' target=\"_blank\">https://wandb.ai/nlp_2023_romibomi_und_paulus_maximus/my-awesome-project/runs/gd2gmm3z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerome/anaconda3/envs/umlia23/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7795' max='7795' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7795/7795 02:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.227700</td>\n",
       "      <td>2.478909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.584600</td>\n",
       "      <td>2.375520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.911500</td>\n",
       "      <td>2.318776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.124900</td>\n",
       "      <td>2.317183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.536100</td>\n",
       "      <td>2.332911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='174' max='174' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [174/174 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▁▁▂▁</td></tr><tr><td>eval/runtime</td><td>█▄▁▁▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▅██▇▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▅██▇▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▃▆▆▃█▅▂▃▄▂▃▂▂▂▄▁▄▂▄▃▃▂▂▃▂▃▄▃▂▁▂▃▂▅▃▂▄▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.31718</td></tr><tr><td>eval/runtime</td><td>0.8881</td></tr><tr><td>eval/samples_per_second</td><td>195.925</td></tr><tr><td>eval/steps_per_second</td><td>195.925</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>7795</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5361</td></tr><tr><td>train/total_flos</td><td>113043110952960.0</td></tr><tr><td>train/train_loss</td><td>2.33718</td></tr><tr><td>train/train_runtime</td><td>167.1615</td></tr><tr><td>train/train_samples_per_second</td><td>46.632</td></tr><tr><td>train/train_steps_per_second</td><td>46.632</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-sponge-2</strong> at: <a href='https://wandb.ai/nlp_2023_romibomi_und_paulus_maximus/my-awesome-project/runs/gd2gmm3z' target=\"_blank\">https://wandb.ai/nlp_2023_romibomi_und_paulus_maximus/my-awesome-project/runs/gd2gmm3z</a><br/>Synced 6 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230712_154033-gd2gmm3z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start wandb run\n",
    "wandb.init()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate()\n",
    "\n",
    "# end wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What languages do they evaluate their methods on?\n",
      "Context: Rumour detection is hard because the most accurate systems operate retrospectively, only recognising rumours once they have collected repeated signals. By then the rumours might have already spread and caused harm. We introduce a new category of features based on novelty, tailored to detect rumours early on. To compensate for the absence of repeated signals, we make use of news wire as an additional data source. Unconfirmed (novel) information with respect to the news articles is considered as an indication of rumours. Additionally we introduce pseudo feedback, which assumes that documents that are similar to previous rumours, are more likely to also be a rumour. Comparison with other real-time approaches shows that novelty based features in conjunction with pseudo feedback perform significantly better, when detecting rumours instantly after their publication.\n",
      "Generated Answer: English\n",
      "Actual Answer: Chinese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerome/anaconda3/envs/umlia23/lib/python3.9/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# run inference on a sample from the test split of flattened dataset\n",
    "sample = flattened_dataset['test'][9]\n",
    "\n",
    "# preprocess the sample\n",
    "preprocessed_sample = preprocess_function(sample, max_qc_len=128, max_ans_len=32)\n",
    "\n",
    "# move the sample to the device (GPU, if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# run inference\n",
    "# model_base = T5ForConditionalGeneration.from_pretrained(\"google/t5-efficient-mini\")\n",
    "# model_base.to(device)\n",
    "generated_answer_ids = model.generate(\n",
    "    input_ids=torch.tensor([preprocessed_sample['input_ids']]).to(device),\n",
    "    attention_mask=torch.tensor([preprocessed_sample['attention_mask']]).to(device),\n",
    ")\n",
    "\n",
    "# decode the generated answer ids\n",
    "generated_answer = tokenizer.decode(generated_answer_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Question:\", sample['question'])\n",
    "print(\"Context:\", sample['abstract'])\n",
    "print(\"Generated Answer:\", generated_answer)\n",
    "print(\"Actual Answer:\", sample['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umlia23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
